{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Building Machine Learning Classifiers.***\n",
    "### **Machine Learning**\n",
    "- *The field of study that gives computers the ability to learn without being explicitly programmed*\n",
    "- *A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.*\n",
    "- *Algorithms that 'can figure out how to perform important tasks by generalizing from examples'*\n",
    "- *Practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world*\n",
    "\n",
    "#### **Two Broad Types of Machine Learning**\n",
    "- `Supervised Learning` - *Task of inferring a function from labeled training data to make predictions on useen data*\n",
    "    - example: Predict whether any given email is spam based on known information about the email.\n",
    "- `Unsupervised Learning` - *Deriving structure from data where we don't know the effect of any of the variables*\n",
    "    - example: Based on the content of an email, group similar emails together in distinct folders.\n",
    "    \n",
    "#### **Holdout Test Set**\n",
    "- Sample of data not used in fitting a model for the purpose of evaluationg the model's ability to generalize unseen data.\n",
    "\n",
    "#### ***K-Fold Cross-Validation***\n",
    "- This is used to evalate the model\n",
    "- Full dataset is divided into k-subsets and the holdout method is repeated k times. Each time, one of the k-subsets is used as the test set and the other k-1 subsets are put together to be used to train the model. \n",
    "- This gives a more robust read on the performance of the model rather than just having one single hold out test set.<br>\n",
    "\n",
    "**Illustration:**<br>\n",
    "<img src='five-fold cross validation.png'>\n",
    "\n",
    "#### ***Evaluation Metrics***\n",
    "\n",
    "- **$Accuracy = \\frac{\\#\\ predicted\\ correctly}{total\\ \\#\\ of\\ observations}$**\n",
    "- **$Precision = \\frac{\\#\\ predicted\\ as\\ spam\\ that\\ are\\ actually\\ spam [True\\ Positive]}{total\\ \\#\\ predicted\\ as\\ spam}$**\n",
    "- **$Recall = \\frac{\\#\\ predicted\\ as\\ spam\\ that\\ are\\ actually\\ spam [True\\ Positive]}{total\\ \\#\\ that\\ are\\ actually\\ spam}$**\n",
    "<img src='Precision-Recall.png'>\n",
    "\n",
    "> ***Precision-Recall give you the ability to kind of tailor the aggressiveness of your algorithm basead on your business problem.***\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fitting a Machine Learning model on text data.**\n",
    "> #### ***`Ensemble Method`***\n",
    "> - ***Technique that creates multiple models and then combines them to produce better results than any of the single models individually.***\n",
    "> - Idea is to ***`combine a lot of weak models to create a single strong model.`***\n",
    "> - Basic idea - This leverages the ***aggregate opinion of many over the isolated opinion*** of one. \n",
    ">\n",
    "> <img src='Ensemble-method.png'>\n",
    "> <hr>\n",
    "\n",
    "> #### ***`Random Forest`*** \n",
    "> - ***Ensemble learning method that constructs a `collection of decision trees` and then aggregates the predictions of each tree to determine the final prediction***\n",
    "\n",
    "> ***`Benefits` of Ensemble Methods***\n",
    "> - *Can be used for classification or regression.*\n",
    "> - *Easily handles outliers, missing values, etc.*\n",
    "> - *Accepts various types of inputs (continous, ordinal, etc.)*\n",
    "> - *Less likely to overfit*\n",
    "> - *Outputs feature importance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **Building a basic Random Forest model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "      <th>8104</th>\n",
       "      <th>8105</th>\n",
       "      <th>8106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct_percent    0    1    2    3    4    5    6    7  ...  8097  \\\n",
       "0       160            2.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1       128            4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "2        49            4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "3        62            3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "4        28            7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "   8098  8099  8100  8101  8102  8103  8104  8105  8106  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8109 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in and clean text\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None)\n",
    "data.columns = ['label', 'text_body']\n",
    "\n",
    "# fuction for creating new feature to get percentage of punctuations in text body.\n",
    "def punct_percentage(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(' ')), 3)*100\n",
    "\n",
    "# create new feature\n",
    "data['body_len'] = data['text_body'].apply(lambda x: len(x)-x.count(' '))\n",
    "data['punct_percent'] = data['text_body'].apply(lambda x: punct_percentage(x))\n",
    "\n",
    "# function to clean text\n",
    "def clean_text(text):\n",
    "    text = ''.join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(token) for token in tokens if token not in stopwords]\n",
    "    return text\n",
    "\n",
    "# vectorization\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['text_body'])\n",
    "\n",
    "# concate this vectorized features with the new made features\n",
    "X_features = pd.concat([data['body_len'], \n",
    "                        data['punct_percent'], \n",
    "                        pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Explore RandomForestClassifier Attributes & Hyperparameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_estimator_type', '_get_param_names', '_get_tags', '_make_estimator', '_more_tags', '_required_parameters', '_set_oob_score', '_validate_X_predict', '_validate_estimator', '_validate_y_class_weight', 'apply', 'decision_path', 'feature_importances_', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params']\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(dir(RandomForestClassifier))\n",
    "print(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Explore RandomForestClassifier through Cross-Validation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97755835, 0.97845601, 0.97396768, 0.96585804, 0.97394429])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)      #n_jobs=-1 => to run jobs in parallel. here decesion trees will be built in parallel processes\n",
    "k_folds = KFold(n_splits=5)     #5-folds\n",
    "cross_val_score(rf, X_features, data['label'], cv=k_folds, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### ***Random Forest on a holdout test set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.07768423141760826, 'body_len'),\n",
       " (0.05079383461068237, 7353),\n",
       " (0.034540120207089194, 3135),\n",
       " (0.0318269854549568, 1804),\n",
       " (0.03160939230385166, 5727),\n",
       " (0.03122544832610075, 4799),\n",
       " (0.023014978482740363, 6749),\n",
       " (0.022408596703639518, 2032),\n",
       " (0.017864758929738024, 690),\n",
       " (0.017536690722193132, 392)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print top 10 important features\n",
    "\n",
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]\n",
    "# model.feature_importances_ gives a list without feature names, hence we zipped column names with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on X_test\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.62 / Accuracy: 0.944\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, and accuracy\n",
    "\n",
    "print(f'Precision: {round(precision,3)} / Recall: {round(recall,3)} / Accuracy: {round(sum(y_pred==y_test)/len(y_pred), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img src='PR-results1.png'>\n",
    ">\n",
    "> - ***Thus from the `recall` we get that the predicted `spam` messages out of `all original spams` are only `62%`, and thus this model is `not great` as `38% of the original spam messages are predicted as ham.`***\n",
    "> - *And hence, the amount of spams that are classified as hams, tells us that our model is* ***not good enough in identifying spam***\n",
    ">\n",
    "> <hr>\n",
    "\n",
    "> #### **Random Forest** model with ***`Grid-Search`***\n",
    "> ***`Grid-search`***: **Exhaustively search all parameters combinations in a given grid to determine the best model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Build `OWN` Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training and evaluation, will help while grid-search\n",
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1, random_state=42)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')    \n",
    "    print(f'Est: {n_est} / Max_depth: {depth} <====> Precision: {round(precision,3)} / Recall: {round(recall,3)} / Accuracy: {round(sum(y_pred==y_test)/len(y_pred), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Max_depth: 10 <====> Precision: 1.0 / Recall: 0.234 / Accuracy: 0.885\n",
      "Est: 10 / Max_depth: 20 <====> Precision: 1.0 / Recall: 0.599 / Accuracy: 0.94\n",
      "Est: 10 / Max_depth: 30 <====> Precision: 0.991 / Recall: 0.689 / Accuracy: 0.952\n",
      "Est: 10 / Max_depth: None <====> Precision: 1.0 / Recall: 0.796 / Accuracy: 0.969\n",
      "Est: 50 / Max_depth: 10 <====> Precision: 1.0 / Recall: 0.234 / Accuracy: 0.885\n",
      "Est: 50 / Max_depth: 20 <====> Precision: 1.0 / Recall: 0.659 / Accuracy: 0.949\n",
      "Est: 50 / Max_depth: 30 <====> Precision: 1.0 / Recall: 0.76 / Accuracy: 0.964\n",
      "Est: 50 / Max_depth: None <====> Precision: 1.0 / Recall: 0.868 / Accuracy: 0.98\n",
      "Est: 100 / Max_depth: 10 <====> Precision: 1.0 / Recall: 0.21 / Accuracy: 0.882\n",
      "Est: 100 / Max_depth: 20 <====> Precision: 1.0 / Recall: 0.665 / Accuracy: 0.95\n",
      "Est: 100 / Max_depth: 30 <====> Precision: 1.0 / Recall: 0.754 / Accuracy: 0.963\n",
      "Est: 100 / Max_depth: None <====> Precision: 0.986 / Recall: 0.868 / Accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "for n_est in [10,50,100]:\n",
    "    for depth in [10,20,30,None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we can select the hyperparameters that are best suited for our results/ business needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### ***Random Forest*** with ***`GridSearchCV`***\n",
    "> - ***`Grid-search`***: *Exhaustively search all parameters combinations in a given grid to determine the best model.*\n",
    "> - ***`Cross-Validation`***: *Divide a dataset into k subsets and repeat the holdout method k times where a different subset is used as the holdout set in each iteration.*\n",
    ">\n",
    "> Thus GridSearchCV is the combination of Grid-search and Cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and clean text\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None)\n",
    "data.columns = ['label', 'text_body']\n",
    "\n",
    "# fuction for creating new feature to get percentage of punctuations in text body.\n",
    "def punct_percentage(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(' ')), 3)*100\n",
    "\n",
    "# create new feature\n",
    "data['body_len'] = data['text_body'].apply(lambda x: len(x)-x.count(' '))\n",
    "data['punct_percent'] = data['text_body'].apply(lambda x: punct_percentage(x))\n",
    "\n",
    "# function to clean text\n",
    "def clean_text(text):\n",
    "    text = ''.join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(token) for token in tokens if token not in stopwords]\n",
    "    return text\n",
    "\n",
    "# vectorization\n",
    "# Tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['text_body'])\n",
    "X_tfidf_feat = pd.concat([data['body_len'], \n",
    "                        data['punct_percent'], \n",
    "                        pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "\n",
    "# CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_count = count_vect.fit_transform(data['text_body'])\n",
    "X_count_feat = pd.concat([data['body_len'], \n",
    "                        data['punct_percent'], \n",
    "                        pd.DataFrame(X_count.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Exploring parameter settings using `GridSearchCV`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# defining a grid of parameters for grid-search\n",
    "param = {'n_estimators': [10,150,300,400],\n",
    "        'max_depth': [30,60,90,None]} \n",
    "\n",
    "# create GridSearchCV object, you need to explicitly train this object\n",
    "gs = GridSearchCV(rf, param_grid=param, cv=5, n_jobs=-1)\n",
    "gs_tfidf_fit = gs.fit(X_tfidf_feat, data['label'])\n",
    "\n",
    "# print out the top 5 models from 'df.cs_results_' attribute in gridsearchcv\n",
    "pd.DataFrame(gs_tfidf_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> results:\n",
    "> <img src='gs_cv_tfidf_results.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GridSearchCV object, you need to explicitly train this object\n",
    "gs = GridSearchCV(rf, param_grid=param, cv=5, n_jobs=-1)\n",
    "gs_count_fit = gs.fit(X_tfidf_feat, data['label'])\n",
    "\n",
    "# print out the top 5 models from 'df.cs_results_' attribute in gridsearchcv\n",
    "pd.DataFrame(gs_count_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> results:\n",
    "> <img src='gs_cv_countvect_results.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In practise, we should experiement a lot. Try out different feature engineering steps, or make good features, try different vectorizers, try for different ML models, blah blah blah..... many things have to be done. All this was just a small example of how to fit and in what steps to create a NLP based model.\n",
    "> <hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### ***`Gradient Boosting`***\n",
    "> - **Ensemble learning method** that takes an ***iterative approach to `combining weak learners` to create a `strong learner` by `focusing on mistakes of prior iterations`.*** \n",
    "> - Uses decision trees as well, but they're incredibly basic, like a ***`decision stump.`***\n",
    "> - It evaluates what it gets right and what it gets wrong on that first tree, and then with the next iteration it places a ***`heavier weight` on those observations that it got wrong*** and it does this over and over and over again focusing on the examples it doesn't quite understand yet until it has minimized the error as much as possible.\n",
    "\n",
    "> **How are RF and Gradient Boosting different?**\n",
    "> <img src='rf-vs-gb.png'>\n",
    "\n",
    "> ***`Bagging` vs `Boosting`*** \n",
    ">\n",
    "> |Bagging|Boosting|\n",
    "> |:-----:|:------:|\n",
    "> |Samples Randomly|Samples with an increased weight on the ones that it got wrong previously|\n",
    "> |Can be parallalized|Can't be parallalized, it is iterative. Relies on the tree before it|\n",
    "> |Less time taken to train|Huge amount of time taken for training job|\n",
    "\n",
    "> ***Gradient Boosting is typically more powerful and better-performing if tuned properly***\n",
    "\n",
    "> **Trade-Offs of Gradient Boosting**:\n",
    "> - `Pros`\n",
    ">    - Extremely powerful\n",
    ">    - Accepts various types of inputs\n",
    ">    - Can be used for classification or regression\n",
    ">    - Outputs feature importance\n",
    "> - `Cons`\n",
    ">    - Longer to train (can't parallelize)\n",
    ">    - More likely to overfit\n",
    ">    - More difficult to properly tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **Explore Gradient Boosting model with grid-search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "      <th>8104</th>\n",
       "      <th>8105</th>\n",
       "      <th>8106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct_percent    0    1    2    3    4    5    6    7  ...  8097  \\\n",
       "0       160            2.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1       128            4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "2        49            4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "3        62            3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "4        28            7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "   8098  8099  8100  8101  8102  8103  8104  8105  8106  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8109 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in and clean text\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None)\n",
    "data.columns = ['label', 'text_body']\n",
    "\n",
    "# fuction for creating new feature to get percentage of punctuations in text body.\n",
    "def punct_percentage(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(' ')), 3)*100\n",
    "\n",
    "# create new feature\n",
    "data['body_len'] = data['text_body'].apply(lambda x: len(x)-x.count(' '))\n",
    "data['punct_percent'] = data['text_body'].apply(lambda x: punct_percentage(x))\n",
    "\n",
    "# function to clean text\n",
    "def clean_text(text):\n",
    "    text = ''.join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(token) for token in tokens if token not in stopwords]\n",
    "    return text\n",
    "\n",
    "# vectorization\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['text_body'])\n",
    "\n",
    "# concate this vectorized features with the new made features\n",
    "X_features = pd.concat([data['body_len'], \n",
    "                        data['punct_percent'], \n",
    "                        pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_SUPPORTED_LOSS', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_check_initialized', '_check_params', '_clear_state', '_compute_partial_dependence_recursion', '_estimator_type', '_fit_stage', '_fit_stages', '_get_param_names', '_get_tags', '_init_state', '_is_initialized', '_make_estimator', '_more_tags', '_raw_predict', '_raw_predict_init', '_required_parameters', '_resize_state', '_staged_raw_predict', '_validate_estimator', '_validate_y', 'apply', 'decision_function', 'feature_importances_', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params', 'staged_decision_function', 'staged_predict', 'staged_predict_proba']\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print(dir(GradientBoostingClassifier))\n",
    "print(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built our own Grid-search\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GB(est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')    \n",
    "    print(f'Est: {n_est} / Max_depth: {max_depth} / LR: {lr} <====> Precision: {round(precision,3)} / Recall: {round(recall,3)} / Accuracy: {round(sum(y_pred==y_test)/len(y_pred), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilesh/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 50 / Max_depth: 3 / LR: 0.01 <====> Precision: 0.0 / Recall: 0.0 / Accuracy: 0.851\n",
      "Est: 50 / Max_depth: 3 / LR: 0.1 <====> Precision: 0.959 / Recall: 0.705 / Accuracy: 0.952\n",
      "Est: 50 / Max_depth: 3 / LR: 1 <====> Precision: 0.901 / Recall: 0.771 / Accuracy: 0.953\n",
      "Est: 50 / Max_depth: 7 / LR: 0.01 <====> Precision: 1.0 / Recall: 0.006 / Accuracy: 0.852\n",
      "Est: 50 / Max_depth: 7 / LR: 0.1 <====> Precision: 0.947 / Recall: 0.753 / Accuracy: 0.957\n",
      "Est: 50 / Max_depth: 7 / LR: 1 <====> Precision: 0.899 / Recall: 0.807 / Accuracy: 0.958\n",
      "Est: 50 / Max_depth: 11 / LR: 0.01 <====> Precision: 1.0 / Recall: 0.018 / Accuracy: 0.854\n",
      "Est: 50 / Max_depth: 11 / LR: 0.1 <====> Precision: 0.928 / Recall: 0.777 / Accuracy: 0.958\n",
      "Est: 50 / Max_depth: 11 / LR: 1 <====> Precision: 0.918 / Recall: 0.807 / Accuracy: 0.961\n",
      "Est: 100 / Max_depth: 3 / LR: 0.01 <====> Precision: 0.966 / Recall: 0.512 / Accuracy: 0.925\n",
      "Est: 100 / Max_depth: 3 / LR: 0.1 <====> Precision: 0.961 / Recall: 0.741 / Accuracy: 0.957\n",
      "Est: 100 / Max_depth: 3 / LR: 1 <====> Precision: 0.91 / Recall: 0.789 / Accuracy: 0.957\n",
      "Est: 100 / Max_depth: 7 / LR: 0.01 <====> Precision: 0.972 / Recall: 0.639 / Accuracy: 0.943\n",
      "Est: 100 / Max_depth: 7 / LR: 0.1 <====> Precision: 0.955 / Recall: 0.771 / Accuracy: 0.961\n",
      "Est: 100 / Max_depth: 7 / LR: 1 <====> Precision: 0.887 / Recall: 0.801 / Accuracy: 0.955\n",
      "Est: 100 / Max_depth: 11 / LR: 0.01 <====> Precision: 0.959 / Recall: 0.711 / Accuracy: 0.952\n",
      "Est: 100 / Max_depth: 11 / LR: 0.1 <====> Precision: 0.917 / Recall: 0.801 / Accuracy: 0.96\n",
      "Est: 100 / Max_depth: 11 / LR: 1 <====> Precision: 0.905 / Recall: 0.807 / Accuracy: 0.959\n",
      "Est: 150 / Max_depth: 3 / LR: 0.01 <====> Precision: 0.967 / Recall: 0.524 / Accuracy: 0.926\n",
      "Est: 150 / Max_depth: 3 / LR: 0.1 <====> Precision: 0.954 / Recall: 0.753 / Accuracy: 0.958\n",
      "Est: 150 / Max_depth: 3 / LR: 1 <====> Precision: 0.921 / Recall: 0.771 / Accuracy: 0.956\n",
      "Est: 150 / Max_depth: 7 / LR: 0.01 <====> Precision: 0.965 / Recall: 0.663 / Accuracy: 0.946\n",
      "Est: 150 / Max_depth: 7 / LR: 0.1 <====> Precision: 0.95 / Recall: 0.801 / Accuracy: 0.964\n",
      "Est: 150 / Max_depth: 7 / LR: 1 <====> Precision: 0.903 / Recall: 0.789 / Accuracy: 0.956\n",
      "Est: 150 / Max_depth: 11 / LR: 0.01 <====> Precision: 0.954 / Recall: 0.753 / Accuracy: 0.958\n",
      "Est: 150 / Max_depth: 11 / LR: 0.1 <====> Precision: 0.931 / Recall: 0.807 / Accuracy: 0.962\n",
      "Est: 150 / Max_depth: 11 / LR: 1 <====> Precision: 0.905 / Recall: 0.807 / Accuracy: 0.959\n"
     ]
    }
   ],
   "source": [
    "for n_est in [50,100,150]:\n",
    "    for max_depth in [3,7,11]:\n",
    "        for lr in [0.01,0.1,1]:\n",
    "            train_GB(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Insights**<br>\n",
    "> <img src='poorly-performing-gb-gs-model.png'>\n",
    "> <img src='best-performing-gb-gs-models.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **Evaluate Gradient Boosting with GridSearchCV**\n",
    "> As we used GridSearchCV with RF, similarly we can use Gradinet boosting with `GridSearchCV`\n",
    "\n",
    "> Here is just the syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifierdientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100,150],\n",
    "    'max_depth': [7,11,15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "\n",
    "# fitting data for tfidf vectorized data (X_tfidf_feat)\n",
    "cv_fit = gs.fit(X_tfidf_feat, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]\n",
    "\n",
    "# fitting data for count-vectorized data (X_count_feat)\n",
    "cv_fit = gs.fit(X_count_feat, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### ***`Final Model Selection`***\n",
    "> Now that we have completed the entire pipeline except for `final model selection`, we would tweak this final process a bit.\n",
    "> ### ***Process*** (`new/tweaked`)\n",
    "> - *Split the data* into training and test set.\n",
    "> - *Train vectorizers on training set* and use that to *transform test set*. [IMPORTANT CHANGE]\n",
    "> - Fit best ramdom forest model and best gradient boosting model on training set and *predict on test set*.\n",
    "> - Thoroughly evaluate results of these two models to *select best model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and clean text\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>7135</th>\n",
       "      <th>7136</th>\n",
       "      <th>7137</th>\n",
       "      <th>7138</th>\n",
       "      <th>7139</th>\n",
       "      <th>7140</th>\n",
       "      <th>7141</th>\n",
       "      <th>7142</th>\n",
       "      <th>7143</th>\n",
       "      <th>7144</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  7135  7136  \\\n",
       "0        63     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        31     6.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2       129     3.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        21     4.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4        95     3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   7137  7138  7139  7140  7141  7142  7143  7144  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 7147 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizing\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.879 / Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.907 / Recall: 0.83 / Accuracy: 0.968\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Checking the `time taken to train` and evaluate for both the models<br>\n",
    "> This becomes a major factor for `model selection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 2.616 / Predict time: 0.191 ---- Precision: 1.0 / Recall: 0.865 / Accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 128.681 / Predict time: 0.058 ---- Precision: 0.902 / Recall: 0.844 / Accuracy: 0.969\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Insights***<br>\n",
    "> - For final comparision we should ideally compare `fit time`, `predict time`, `precision`, `recall`, and `accuracy` between the two models. with a particular *focus* on **predict time, precision and recall**<br>\n",
    "> - Even if the **Gradient Boosting model** takes a lot `more time to train` but it takes much `less time to predict`. Thus working at real time, this becomes a major reason to select GradientBoosting model.<br>\n",
    "> - RF has better precision, but GB has better recall.\n",
    "> - Now final selection has to done based on the business needs.\n",
    "\n",
    "> ### **Two Final Points**\n",
    "> - Further evaluation\n",
    ">    - Slice test set\n",
    ">    - Examine text messages the model is getting wrong\n",
    "> - **Result trade-off**\n",
    ">    - Make decisions based on **business problem or the business context**\n",
    ">    - Is predict time of 0.213 vs. 0.135 going to create a bottleneck?\n",
    ">    - Precision/recall\n",
    ">       - Spam filter- optimize for precision (when it says spam, it better be spam)\n",
    ">       - Antivirus software- optimize for recall\n",
    "\n",
    "> In our case, RF has better precision, very close recall to that of GB and predict time is not huge, hence we can easily **select RF as the final model** here!!! Yaay!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
